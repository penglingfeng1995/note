# 副本

配置副本需要启动zookeeper，不同的clickhouse实例互为主从

修改 配置文件

```bash
vim /etc/clickhouse-server/config.xml
```

找到以下标签进行配置zookeeper

```xml
<zookeeper>
    <node>
        <host>192.169.0.238</host>
        <port>2184</port>
    </node>
</zookeeper>
```

然后找到以下标签<interserver_http_port>，<interserver_http_host>放开注释，这里是用来副本间传数据的地址端口，端口默认配好了是9009，防火墙需要放开这个端口，地址需要配置下指定当前主机具体的地址，不能是127.0.0.1 ，localhost之类的，不然别的主机读不到这台机。

```xml
<interserver_http_port>9009</interserver_http_port>

<interserver_http_host>192.168.x.x</interserver_http_host>
```

重启服务

```bash
clickhouse restart
```

创建表，注意引擎需使用MergeTree的ReplicatedMergeTree，副本是表级别的，互为副本的表，需指定同一个zk的路径存放表信息，和一个不同的副本名。

```sql
create table t_order(
    id Int32,
    name String
)engine = ReplicatedMergeTree('/clickhouse/table/01/t_order','t1')
order by id;

create table t_order(
    id Int32,
    name String
)engine = ReplicatedMergeTree('/clickhouse/table/01/t_order','t2')
order by id;
```



# 分片集群

clickhouse的分片集群是表级别的，通过一个分布式表，来调度其他的实体表，分布式表本身不存储数据，类似mycat，分片集群的各个表是根据一定算法接受数据。

在 config.xml 中，找到 <remote_servers> 标签，在里面配置集群配置

```xml
<my_cluster>
    <shard>
        <replica>
            <host>192.168.0.1</host>
            <port>9000</port>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>192.168.0.2</host>
            <port>9000</port>
        </replica>
    </shard>
</my_cluster>
```

为了方便可以配置变量，在 marcos 标签里指定，后续建表可以使用占位符，创建时则替换为变量，变量名和值都可以自定义。

```xml
<macros>
    <shard>01</shard>
    <replica>example01-01-1</replica>
</macros>
```

重启clickhouse，并暴露9000端口，该端口用于分布式搜索。

执行创建表语句，第一行选择在集群上创建`on cluster`，执行一次，每个节点都会创建，引擎里可以使用`{}` 占位符，来调用我们macros里之前定义的变量。

```sql
create table t_book on cluster my_cluster (
    id Int32,
    name String,
    create_time DateTime
)engine = ReplicatedMergeTree('/clickhouse/table/{shard}/t_book','{replica}')
order by id;
```

然后创建分布式表，同样是在集群上创建，引擎选分布式，参数为：集群名，库名，表名，分片键。

分片键需为数字整型，可以使用函数转换下。

```sql
create table t_book_all on cluster my_cluster(
        id Int32,
    name String,
    create_time DateTime
)engine = Distributed('my_cluster',default,t_book,id);
```

插入数据测试

```sql
insert into t_book_all values
(1,'zs','2022-02-22 14:57:43')
(2,'ls','2022-02-22 14:57:43')
(3,'ww','2022-02-22 14:57:43')
(4,'zl','2022-02-22 14:57:43')
(5,'al','2022-02-22 14:57:43')
(6,'am','2022-02-22 14:57:43');

select * from t_book_all;

// 246,135
select * from t_book;
```

可以看到，通过分布式表操作插入，数据会按照分片键，存储到不同的分片节点中。

## 集群扩容

新部署的clickhouse ，配置集群，zk ，

旧的节点，也要编辑，把新分片配置添加上

```xml
<my_cluster>
    <shard>
        <replica>
            <host>192.168.0.1</host>
            <port>9000</port>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>192.168.0.2</host>
            <port>9000</port>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>192.168.0.3</host>
            <port>9000</port>
        </replica>
    </shard>
</my_cluster>
```

以及 marcos变量，marcos变量需要唯一，集群配置和zk需一致。

全部重启，检查集群信息是否同步。

```sql
select * from system.clusters where cluster = 'my_cluster';
```

重新在新节点上创建表，此时不能使用 `on cluster` 语法，本机单独创建即可，否则与旧节点冲突

```sql
create table t_book (
    id Int32,
    name String,
    create_time DateTime
)engine = ReplicatedMergeTree('/clickhouse/table/{shard}/t_book','{replica}')
order by id;

create table t_book_all (
        id Int32,
    name String,
    create_time DateTime
)engine = Distributed('my_cluster',default,t_book,id);
```

此时进行操作，集群已经生效，新节点可用。

可以在 `<shard>` 节点下配置每个节点的权重，约大权重越高，通常给新节点配置大权重，吸收新数据。

```xml
<weight>1</weight>
```

